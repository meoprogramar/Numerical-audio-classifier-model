{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassificaÃ§Ã£o de nÃºmeros atravÃ©s de Ã¡udios\n",
    "O algorimo tem como objetivo desenvolver um modelo que consiga classificar nÃºmeros atravÃ©s de entrada de Ã¡udio, para isso foi usado o dataset [__MNIST__](https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist). Basicamente o dataset contÃ©m __30000 Ã¡udios__ com o som dos nÃºmeros de 0 atÃ© 9 falados por 60 diferentes pessoas. \n",
    "\n",
    "Para a resoluÃ§Ã£o do problema foi escolhido o uso de __Redes Neurais Artificiais__, ao final do processo serÃ¡ possÃ­vel entrar com um Ã¡udio e o modelo classificarÃ¡ e informarÃ¡ qual o nÃºmero dito.\n",
    "\n",
    "### Estrutura do projeto\n",
    "\n",
    "<ul>\n",
    "  <li>ðŸ“‚ <strong>dataset</strong>\n",
    "    <ul>\n",
    "      <li>ðŸ“‚ <strong>audios</strong>: Pasta com os Ã¡udios do dataset, extraia os Ã¡udios para essa pasta.</li>\n",
    "      <li>ðŸ“‚ <strong>audios_extras</strong>: Pasta com os Ã¡udios extras, gravados manualmente por mim para testar o modelo.</li>\n",
    "      <li>ðŸ“‚ <strong>audios_processed</strong>: Pasta com os Ã¡udios processados em csv caso vocÃª nÃ£o queira baixar o dataset, tambÃ©m jÃ¡ estÃ¡ separado em dados de treino e teste.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>.gitignore</li>\n",
    "  <li>audio_processing.ipynb</li>\n",
    "</ul>\n",
    "\n",
    "__OBS: O dataset nÃ£o estÃ¡ incluso no repositÃ³rio devido a limites do github, faÃ§a o download do dataset [__aqui__](https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist). Em seguida extraia os Ã¡udios para a pasta \"/dataset/audios/...\" ficando como mostrado abaixo:__\n",
    "\n",
    "<ul>\n",
    "  <li>ðŸ“‚ <strong>dataset</strong>\n",
    "    <ul>\n",
    "      <li>ðŸ“‚ <strong>audios</strong>\n",
    "      <ul>\n",
    "        <li>ðŸ“‚ <strong>01</strong></li>\n",
    "        <li>ðŸ“‚ <strong>02</strong></li>\n",
    "        <li>ðŸ“‚ <strong>03</strong></li>\n",
    "        <li>ðŸ“‚ <strong>...</strong></li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Bibliotecas usadas\n",
    "\n",
    "Foram usadas bibliotecas padrÃµes para manipulaÃ§Ã£o de dados como __pandas e numpy__, para a manipulaÃ§Ã£o de entradas de Ã¡udio foi usado as libs __glob e librosa__, por fim, usou-se bibliotecas para desenvolvimento do modelo como __sklearn e keras__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. FunÃ§Ãµes utilitÃ¡rias\n",
    "\n",
    "Foram definidas algumas funÃ§Ãµes bÃ¡sicas para ajudar no desenvolvimento.\n",
    "\n",
    "__describe_audio__: Fornece um resumo do Ã¡udio, exibindo um reprodutor e o grÃ¡fico do mesmo.<br>\n",
    "__get_audio_mfcc__: Retorna o Mel-frequency cepstral coeficiente do Ã¡udio.<br>\n",
    "__get_audio_class__: Retorna qual a classe do Ã¡udio.<br>\n",
    "__extract_audio_data__: Extrai de um array de Ã¡udios os dados do Ã¡udio, retornando o data cru e o data pronto para o modelo.\n",
    "__extract_audio_target__: Extrai de um array de Ã¡udios a classe dos mesmos, retornando as classes cruas e as classes em dummy encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_audio(audio):\n",
    "    y, sr = librosa.load(audio)\n",
    "    pd.Series(y).plot()\n",
    "    return ipd.Audio(audio)\n",
    "\n",
    "def get_audio_mfcc(audio, n_mfcc=50):\n",
    "    y, sr = librosa.load(audio)\n",
    "    mfcc = librosa.feature.mfcc(y, sr, n_mfcc=n_mfcc)\n",
    "    # mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "    return mfcc\n",
    "\n",
    "def get_audio_class(audio):\n",
    "    filename = os.path.basename(audio.title())\n",
    "    return int(filename.split('_')[0])\n",
    "\n",
    "def extract_audio_data(audios):\n",
    "    n_mfcc = 50\n",
    "    raw_data = []\n",
    "    transformed_data = np.empty((0, 50))\n",
    "\n",
    "    for audio_index in range(0, len(audios)):\n",
    "        print('Extracting data from audio... ' + str(audio_index+1) + '/' + str(len((audios)))) \n",
    "        raw_data.append(get_audio_mfcc(audios[audio_index], n_mfcc))\n",
    "\n",
    "    for mfcc in raw_data:\n",
    "        mfcc_transposed = np.mean(mfcc.T, axis=0)\n",
    "        transformed_data = np.vstack((transformed_data, mfcc_transposed.tolist()))\n",
    "\n",
    "    return raw_data, transformed_data\n",
    "\n",
    "def extract_audio_target(audios):\n",
    "    raw_target = []\n",
    "\n",
    "    for audio_index in range(0, len(audios)):\n",
    "        print('Extracting target from audio... ' + str(audio_index+1) + '/' + str(len((audios)))) \n",
    "        raw_target.append(get_audio_class(audios[audio_index]))\n",
    "\n",
    "    transformed_target = pd.get_dummies(raw_target).values\n",
    "\n",
    "    return raw_target, transformed_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Carregamento dos Ã¡udios que serÃ£o usados para treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob('dataset/audios/*/*.wav')\n",
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. TransformaÃ§Ã£o dos Ã¡udios\n",
    "\n",
    "SerÃ¡ gerado nesse passo os previsores e as classes que serÃ£o usados para o treino e teste do modelo. Os previsores (x), sÃ£o extraÃ­dos dos Ã¡udios usando o __Mel-frequency cepstral__ coeficiente, basicamente esse coeficiente consegue resumir o dado em suas caracterÃ­sticas mais importantes do ponto de vista do modelo. JÃ¡ as classes sÃ£o extraÃ­das dos nomes dos arquivos, os primeiros caracteres antes do _ (underline) significa qual a classe do Ã¡udio, por exemplo o Ã¡udio __\"05_01_02.wav\"__, o __05__ inicial siginifica que esse Ã¡udio Ã© o som do nÃºmero cinco.\n",
    "\n",
    "Para treinar o modelo, usaremos as variÃ¡veis com o prefixo __\"transformed...\"__, pois elas jÃ¡ estÃ£o no formato que o modelo irÃ¡ receber.\n",
    "\n",
    "__OBS: Esse processo pode levar alguns minutos...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, transformed_data = extract_audio_data(audio_files)\n",
    "raw_target, transformed_target = extract_audio_target(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. DivisÃ£o dos dados\n",
    "\n",
    "Nesse passo usaremos o __train_test_split__ para realizar a divisÃ£o randÃ´mica entre a base de dados, separando os dados uma parte para treino, outra parte para teste... por padrÃ£o foi definido a divisÃ£o __70/30__, onde __70%__ dos dados serÃ£o usados para treino e __30%__ usados para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, training_target, test_target = train_test_split(transformed_data, transformed_target, test_size=0.3, random_state=0)\n",
    "\n",
    "print(training_data.shape, test_data.shape, training_target.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. CriaÃ§Ã£o do modelo\n",
    "\n",
    "Criou-se o modelo, definido em 4 camadas incluindo a camada de saÃ­da, a camada de entrada contÃ©m 50 perceptrons, pois cada Ã¡udio foi mapeado em 50 colunas. Adicionamos dropout para evitar o overfitting, e usou-se a funÃ§Ã£o de loss do tipo __categorical_crossentropy__.\n",
    "\n",
    "A camada de saÃ­da apresenta 10 perceptrons devido a saÃ­da ser 10 classes, indo de 0 atÃ© 9, e as classes foram mapeadas em one hot encoder, gerando assim 10 colunas de saÃ­da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. VariÃ¡veis e treinamento\n",
    "\n",
    "Definimos que o modelo iria realizar __100 epochs__ e efetuar o balanceamento dos pesos a cada __32 items__... adicionamos tambÃ©m um _check pointer_ para salvar o melhor modelo entre os 100 ciclos. Em seguida, o modelo serÃ¡ treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               save_best_only=True)\n",
    "\n",
    "model.fit(training_data, training_target, \n",
    "          epochs=num_epochs,\n",
    "          validation_data=(test_data, test_target), \n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Teste de precisÃ£o\n",
    "\n",
    "Ao fim do treinamento, rodamos o modelo com os dados separados de teste para fazer uma avaliaÃ§Ã£o da sua performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(test_data, test_target, verbose = 0)\n",
    "print('PrecisÃ£o do modelo Ã© de ' + str(accuracy[1]*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Teste manual\n",
    "\n",
    "Na pasta _dataset/audios_extras_ foi adicionado alguns exemplos de Ã¡udios gravados por mim, para testar a eficiÃªncia do modelo. VocÃª tambÃ©m pode tentar, basta o Ã¡udio ser um arquivo __.wav__ e ter no mÃ¡ximo __1 segundo__, lembrando que o modelo foi treinado com dados limpos e padronizados do dataset usado ([__MNIST__](https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist)), entÃ£o caso vocÃª coloque um Ã¡udio muito grande, muito baixo, ou muito alto, provavelmente o modelo nÃ£o irÃ¡ reconhecer. O objetivo desse algoritmo nÃ£o Ã© ser o melhor, mas sim didÃ¡tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_files = glob('dataset/audios_extras/*.wav')\n",
    "len(test_audio_files)\n",
    "\n",
    "_, test_transformed_data = extract_audio_data(test_audio_files)\n",
    "\n",
    "manual_test_result = model.predict(test_transformed_data)\n",
    "manual_test_result = np.where(manual_test_result > 0.5, 1, 0)\n",
    "\n",
    "for audio_index in range(0, len(test_audio_files)):\n",
    "    print('O Ã¡udio \"' + str(test_audio_files[audio_index].title()) + '\" foi classificado como ' + str(np.argmax(manual_test_result[audio_index])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
