{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifica√ß√£o de n√∫meros atrav√©s de √°udios\n",
    "O algorimo tem como objetivo desenvolver um modelo que consiga classificar n√∫meros atrav√©s de entrada de √°udio, para isso foi usado o dataset [__MNIST__](https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist). Basicamente o dataset cont√©m __30000 √°udios__ com o som dos n√∫meros de 0 at√© 9 falados por 60 diferentes pessoas. \n",
    "\n",
    "Para a resolu√ß√£o do problema foi escolhido o uso de __Redes Neurais Artificiais__, ao final do processo ser√° poss√≠vel entrar com um √°udio e o modelo classificar√° e informar√° qual o n√∫mero dito.\n",
    "\n",
    "### Estrutura do projeto\n",
    "\n",
    "<ul>\n",
    "  <li>üìÇ <strong>dataset</strong>\n",
    "    <ul>\n",
    "      <li>üìÇ <strong>audios</strong>: Pasta com os √°udios do dataset, extraia os √°udios para essa pasta.</li>\n",
    "      <li>üìÇ <strong>audios_extras</strong>: Pasta com os √°udios extras, gravados manualmente por mim para testar o modelo.</li>\n",
    "      <li>üìÇ <strong>audios_processed</strong>: Pasta com os √°udios processados em csv caso voc√™ n√£o queira baixar o dataset, tamb√©m j√° est√° separado em dados de treino e teste.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>.gitignore</li>\n",
    "  <li>audio_processing.ipynb</li>\n",
    "</ul>\n",
    "\n",
    "__OBS: O dataset n√£o est√° incluso no reposit√≥rio devido a limites do github, fa√ßa o download do dataset [__aqui__](https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist). Em seguida extraia os √°udios para a pasta \"/dataset/audios/...\" ficando como mostrado abaixo:__\n",
    "\n",
    "<ul>\n",
    "  <li>üìÇ <strong>dataset</strong>\n",
    "    <ul>\n",
    "      <li>üìÇ <strong>audios</strong>\n",
    "      <ul>\n",
    "        <li>üìÇ <strong>01</strong></li>\n",
    "        <li>üìÇ <strong>02</strong></li>\n",
    "        <li>üìÇ <strong>03</strong></li>\n",
    "        <li>üìÇ <strong>...</strong></li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Bibliotecas usadas\n",
    "\n",
    "Foram usadas bibliotecas padr√µes para manipula√ß√£o de dados como __pandas e numpy__, para a manipula√ß√£o de entradas de √°udio foi usado as libs __glob e librosa__, por fim, usou-se bibliotecas para desenvolvimento do modelo como __sklearn e keras__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Fun√ß√µes utilit√°rias\n",
    "\n",
    "Foram definidas algumas fun√ß√µes b√°sicas para ajudar no desenvolvimento.\n",
    "\n",
    "__describe_audio__: Fornece um resumo do √°udio, exibindo um reprodutor e o gr√°fico do mesmo.<br>\n",
    "__get_audio_mfcc__: Retorna o Mel-frequency cepstral coeficiente do √°udio.<br>\n",
    "__get_audio_class__: Retorna qual a classe do √°udio.<br>\n",
    "__extract_audio_data__: Extrai de um array de √°udios os dados do √°udio, retornando o data cru e o data pronto para o modelo.\n",
    "__extract_audio_target__: Extrai de um array de √°udios a classe dos mesmos, retornando as classes cruas e as classes em dummy encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_audio(audio):\n",
    "    y, sr = librosa.load(audio)\n",
    "    pd.Series(y).plot()\n",
    "    return ipd.Audio(audio)\n",
    "\n",
    "def get_audio_mfcc(audio, n_mfcc=50):\n",
    "    y, sr = librosa.load(audio)\n",
    "    mfcc = librosa.feature.mfcc(y, sr, n_mfcc=n_mfcc)\n",
    "    # mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "    return mfcc\n",
    "\n",
    "def get_audio_class(audio):\n",
    "    filename = os.path.basename(audio.title())\n",
    "    return int(filename.split('_')[0])\n",
    "\n",
    "def extract_audio_data(audios):\n",
    "    n_mfcc = 50\n",
    "    raw_data = []\n",
    "    transformed_data = np.empty((0, 50))\n",
    "\n",
    "    for audio_index in range(0, len(audios)):\n",
    "        print('Extracting data from audio... ' + str(audio_index+1) + '/' + str(len((audios)))) \n",
    "        raw_data.append(get_audio_mfcc(audios[audio_index], n_mfcc))\n",
    "\n",
    "    for mfcc in raw_data:\n",
    "        mfcc_transposed = np.mean(mfcc.T, axis=0)\n",
    "        transformed_data = np.vstack((transformed_data, mfcc_transposed.tolist()))\n",
    "\n",
    "    return raw_data, transformed_data\n",
    "\n",
    "def extract_audio_target(audios):\n",
    "    raw_target = []\n",
    "\n",
    "    for audio_index in range(0, len(audios)):\n",
    "        print('Extracting target from audio... ' + str(audio_index+1) + '/' + str(len((audios)))) \n",
    "        raw_target.append(get_audio_class(audios[audio_index]))\n",
    "\n",
    "    transformed_target = pd.get_dummies(raw_target).values\n",
    "\n",
    "    return raw_target, transformed_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Carregamento dos √°udios que ser√£o usados para treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob('dataset/audios/*/*.wav')\n",
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Transforma√ß√£o dos √°udios\n",
    "\n",
    "Ser√° gerado nesse passo os previsores e as classes que ser√£o usados para o treino e teste do modelo. Os previsores (x), s√£o extra√≠dos dos √°udios usando o __Mel-frequency cepstral__ coeficiente, basicamente esse coeficiente consegue resumir o dado em suas caracter√≠sticas mais importantes do ponto de vista do modelo. J√° as classes s√£o extra√≠das dos nomes dos arquivos, os primeiros caracteres antes do _ (underline) significa qual a classe do √°udio, por exemplo o √°udio __\"05_01_02.wav\"__, o __05__ inicial siginifica que esse √°udio √© o som do n√∫mero cinco.\n",
    "\n",
    "Para treinar o modelo, usaremos as vari√°veis com o prefixo __\"transformed...\"__, pois elas j√° est√£o no formato que o modelo ir√° receber.\n",
    "\n",
    "__OBS: Esse processo pode levar alguns minutos...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, transformed_data = extract_audio_data(audio_files)\n",
    "raw_target, transformed_target = extract_audio_target(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Divis√£o dos dados\n",
    "\n",
    "Nesse passo usaremos o __train_test_split__ para realizar a divis√£o rand√¥mica entre a base de dados, separando os dados uma parte para treino, outra parte para teste... por padr√£o foi definido a divis√£o __70/30__, onde __70%__ dos dados ser√£o usados para treino e __30%__ usados para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, training_target, test_target = train_test_split(transformed_data, transformed_target, test_size=0.3, random_state=0)\n",
    "\n",
    "print(training_data.shape, test_data.shape, training_target.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Cria√ß√£o do modelo\n",
    "\n",
    "Criou-se o modelo, definido em 4 camadas incluindo a camada de sa√≠da, a camada de entrada cont√©m 50 perceptrons, pois cada √°udio foi mapeado em 50 colunas. Adicionamos dropout para evitar o overfitting, e usou-se a fun√ß√£o de loss do tipo __categorical_crossentropy__.\n",
    "\n",
    "A camada de sa√≠da apresenta 10 perceptrons devido a sa√≠da ser 10 classes, indo de 0 at√© 9, e as classes foram mapeadas em one hot encoder, gerando assim 10 colunas de sa√≠da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Vari√°veis e treinamento\n",
    "\n",
    "Definimos que o modelo iria realizar __100 epochs__ e efetuar o balanceamento dos pesos a cada __32 items__... adicionamos tamb√©m um _check pointer_ para salvar o melhor modelo entre os 100 ciclos. Em seguida, o modelo ser√° treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               save_best_only=True)\n",
    "\n",
    "model.fit(training_data, training_target, \n",
    "          epochs=num_epochs,\n",
    "          validation_data=(test_data, test_target), \n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Teste de precis√£o\n",
    "\n",
    "Ao fim do treinamento, rodamos o modelo com os dados separados de teste para fazer uma avalia√ß√£o da sua performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(test_data, test_target, verbose = 0)\n",
    "print('Precis√£o do modelo √© de ' + str(accuracy[1]*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Teste manual\n",
    "\n",
    "Na pasta _dataset/audios_extras_ foi adicionado alguns exemplos de √°udios gravados por mim, para testar a efici√™ncia do modelo. Voc√™ tamb√©m pode tentar, basta o √°udio ser um arquivo __.wav__ e ter no m√°ximo __1 segundo__, lembrando que o modelo foi treinado com dados limpos e padronizados do dataset usado ([__MNIST__](https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist)), ent√£o caso voc√™ coloque um √°udio muito grande, muito baixo, ou muito alto, provavelmente o modelo n√£o ir√° reconhecer. O objetivo desse algoritmo n√£o √© ser o melhor, mas sim did√°tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_files = glob('dataset/audios_extras/*.wav')\n",
    "len(test_audio_files)\n",
    "\n",
    "_, test_transformed_data = extract_audio_data(test_audio_files)\n",
    "\n",
    "manual_test_result = model.predict(test_transformed_data)\n",
    "manual_test_result = np.where(manual_test_result > 0.5, 1, 0)\n",
    "\n",
    "for audio_index in range(0, len(test_audio_files)):\n",
    "    print('O √°udio \"' + str(test_audio_files[audio_index].title()) + '\" foi classificado como ' + str(np.argmax(manual_test_result[audio_index])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
